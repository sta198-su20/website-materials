---
title: "Lab 02"
author: "STA 198 Summer 2020 (Jiang)"
output:
  rmarkdown::html_document:
    css: "./sta198-labs.css"
    toc: TRUE
    toc_float: TRUE
---
<br>

Today's lab will focus on data wrangling using the `tidyverse` (in particular, basic functions in `dplyr`). The lab has a pretty hefty guided portion -- follow along with the code that is written to familiarize yourself with the commands. Don't hesitate to ask your lab TAs or each other for help!

Clone an existing assignment repository into a personal repository and create a new RStudio Cloud Project based on your cloned repository in the class workspace. Knit your final product into a .pdf document and upload this .pdf document to Gradescope by 11:59p US Eastern Time, on June 2nd.

Click on the following link to clone the lab directory: 

[https://classroom.github.com/a/n5mgtlDc](https://classroom.github.com/a/n5mgtlDc)

It may take a bit longer to create the RStudio Cloud project due to the slightly larger dataset.

# NC bike crash data

This week's data come from the North Carolina Department of Transportation. Each year, about 1,000 bicyclists are involved in police-reported crashes with motor vehicles in North Carolina, with 60 serious injuries and 20 deaths. This is a tragic yet preventable public health issue. Today's dataset provides all 7,467 police-reported bicycle crashes involving a motor vehicle in North Carolina from 2007 through 2014 and are found in the `data` folder in the repository.

# Examining our dataset

Last week, we installed the `tidyverse` package, which contains the `dplyr` package for data manipulation. Since this package should already be installed, we only need to load it into memory to access its functions and capabilities. Let's read in the dataset using the `read_csv` function from the `tidyverse`:

```{r load-tidyverse, eval = FALSE}
library(tidyverse)
ncbikecrash <- read_csv("data/ncbikecrash.csv")
```

**Remember**, the R Console and the Markdown document live in separate environments. If you load the `tidyverse` package or the dataset in your console, it won't be loaded into the workspace for knitting the R Markdown document (and vice-versa). The Console is very useful for "testing" code -- seeing if what you input results in what you want. However, for creating reproducible documents, you should be using the R Markdown environment in the Source Pane (the top-left area of your RStudio window).

Suppose we're interested in knowing the names or some basic characteristics of the `ncbikecrash` dataset. This is probably not something we want to include in our final document, but something just for our own curiosity. Try running the following two commands in the Console, and seeing what happens:

```{r names, eval = FALSE}
names(ncbikecrash)
```

```{r glimpse, eval = FALSE}
glimpse(ncbikecrash)
```

The `names()` function returns the list of variable names in the dataset, while the `glimpse` function (part of the `tidyverse`) provides a list of names, the type of variable (integer vs. string vs. double, etc.), as well as the first few observations of each so you get a feel for what some of the data points look like for each variable.

You can also `View` the dataset by using the `View()` function. Try running the following command in the Console:

```{r view, eval = FALSE}
View(ncbikecrash)
```

# Manipulating datasets

`dplyr` is a package that allows users to do some of the most common data manipulation functions. Essentially, pliers for dataframes (hence the name). It is based on the philosophy of functions as **verbs** that manipulate dataframes. Under the rules of `dplyr`, the first argument is always a dataframe, subsequent arguments say what to do with that dataframe, and `dplyr`-based manipulations will always return a data frame. What are some of these verbs that say what to do with our dataframes?

- `filter`: pick rows (observations) matching certain criteria
- `slice`: pick rows (observations) using an index/indices
- `select`: pick columns (variables) by name
- `pull`: grab a column as a vector instead of a data frame
- `arrange`: reorder rows
- `mutate`: add new variables (often used with `casewhen`...more on this later)
- `distinct`: filter for unique rows
- `sample_n`: randomly sample `n` rows
- `sample_frac`: randomly sample a fraction of rows from the dataset
- `summarise` (or `summarize`): reduce variables to summary statistic values
- ...and many more.

As you can see, many of these verbs do what you would expect them to do based on their function name.

# Pipes

`dplyr` functions use a special operator, `%>%`, called a **pipe**. This operator "pipes" the output of the previous line of code as the first input of the next line of code. You can think of pronouncing it as "and then." 

Let's use an everyday example (well, before stay-at-home orders) to illustrate how a pipe works. Consider the following sequence of actions: 1) find car keys, 2) start the car, 3) drive to Duke, and finally, 4) park. if we express this sequence of actions as a set of nested functions in `R` "code," this might look like

```{r nested-functions, eval = FALSE}
park(drive(start_car(find("keys")), to = "campus"))

# ...don't try running this code, it won't work.
```

If we use pipes, we have a more natural and easier to read structure:

```{r pipes, eval = FALSE}
find("keys") %>% 
  start_car() %>% 
  drive(to = "campus") %>% 
  park()

# don't try running this code, either
# still, try to "read" the code in plain English
```

Remember, the `%>%` operator is pronounced "and then," and so the above function using pipes might be pronounced "*find keys, and then start the car, and then drive to campus, and then park*." Much easier to figure out!

# Logical operators in R

In `R`, there are different ways of relating variables. Consider two variables, `x` and `y`.

- `x & y`: `x` AND `y`
- `x | y`: `x` OR `y`
- `!x`: NOT `x`
- `<` and `>`: less than/greater than
- `<=` and `>=`: less than/greater than or equal to
- `==`: exactly equal to
- `!=`: not equal to
- `is.na(x)`: tests if `x` is `NA` and returns a list of `TRUE` or `FALSE` for each element of `x` (similarly, `!is.na(x)` tests for NOT being `NA`)
- `x %in% y`: tests if elements of `x` are in `y` and returns a list of `TRUE` or `FALSE` corresponding to each element of `x`

# Putting it all together

In this section, we're going to walk through some code that examines the logical operators in R. Follow along these steps that make use of some of the functions above.

### filter

We use the `filter` function to select a subset of observations following some criterion(a). Suppose we are only interested in observations in Durham County. Run the following code:

```{r filter-1, eval = FALSE}
ncbikecrash %>%
  filter(county == "Durham")
```

The code should have returned a dataset with 340 rows (observations) and the same 66 variables as before. We use the logical operator `==` to take counties exactly equal to "Durham." Remember that the output of pipe operators is another dataframe (if you are wondering what a `tibble` is, you can think of it as a special dataframe format specific to the `tidyverse`). Suppose we wanted to save the Durham County crashes as a new dataframe, `durham_crashes`. We can use the assignment operator to do that:

```{r filter-2, eval = FALSE}
durham_crashes <- ncbikecrash %>% 
  filter(counter == "Durham")
```

Finally, we can filter for many conditions at once. Suppose we're interested in crashes in Durham County where an ambulance was required, from 2014 onward. We might use the following code:

```{r filter-3, eval = FALSE}
ncbikecrash %>% 
  filter(county == "Durham" & ambulance_req == "Yes" & crash_year >= 2014)
```

We see that there are 28 such observations, and that the previous code returned a `tibble` containing their information.

### slice

Instead of picking observations that match certain criteria, we can simply take a `slice` of the dataset that contains specified row numbers that we want. For instance, let's take the 4th observation:

```{r slice-1, eval = FALSE}
ncbikecrash %>% 
  slice(4)
```

In `R`, the `:` specifies that we want everything from the left side of the colon to the right side. So, `1:8` would be "1 to 8." So, if we wanted to take the first eight observations, we could take the following slice:

```{r slice-2, eval = FALSE}
ncbikecrash %>% 
  slice(1:8)
```

### select

On the other hand, instead of choosing rows (observations), suppose we're interested in choosing columns (variables). We can use the `select` function to do so. Say we want to keep only the `locality` and `speed_limit` variables from the dataframe. Run the following code and see what happens:

```{r select-1, eval = FALSE}
ncbikecrash %>% 
  select(locality, speed_limit)
```

We see that we have a new `tibble` consisting of only the locality and speed limit variables, but for all 7,467 observations. 

The magic of pipes is that we can continue to do more and more operations. Try running the following code, remembering that the pipe operator is pronounced "and then":

```{r select-2, eval = FALSE}
ncbikecrash %>% 
  filter(county == "Durham" & ambulance_req == "Yes" & crash_year >= 2014) %>%
  select(locality, speed_limit)
```

What are we doing? We are taking the original dataset `ncbikecrash`, *and then* filtering observations where the county was Durham AND an ambulance was required AND the crash year was 2014 or greater, *and then* selecting to keep only the variables corresponding to locality and speed limit. The result is a `tibble` containing 28 observations of 2 variables.

We can also use `select` to exclude variables. Suppose we want every variable EXCEPT the "region" variable. We can easily do that by using a minus sign:

```{r select-3, eval = FALSE}
ncbikecrash %>% 
  select(-region)
```

And finally, we don't have to type each variable we want; we can `select` a range of variables at once. The 5th variable in `ncbikecrash` is `development`, and the 17th variable is `bike_direction`. Suppose we want to select those variables and all variables in between. We can use a `:` to do so:

```{r select-4, eval = FALSE}
ncbikecrash %>% 
  select(development:bike_direction)
```

### pull

Sometimes we'd rather have a column as a vector instead of as a dataframe. We can do that with the `pull` function. Compare the following two series of pipe operators by running them in the Console. What is the difference between the two?

```{r pull-1, eval = FALSE}
ncbikecrash %>% 
  slice(5:10) %>% 
  select(locality)
```

```{r pull-2, eval = FALSE}
ncbikecrash %>% 
  slice(5:10) %>% 
  pull(locality)
```

We see that `select` returned a `tibble` of six observations and one variable (locality), whereas `pull` returned a *vector* of the same values. For now, don't worry too much about `pull`, just know that it is an option if you ever need it.

### sample_n and sample_frac

We can also take random samples from our datasets. `sample_n` takes a random sample of `n` observations, whereas `sample_frac` takes a random sample based on the fraction of observations. Both of these functions can sample with or without replacement using the option `replace = TRUE` or `FALSE`.

What is the difference between the following two sets of code?

```{r sample-1, eval = FALSE}
ncbikecrash %>% 
  sample_n(5, replace = FALSE)
```

```{r sample-2, eval = FALSE}
ncbikecrash %>%
  sample_frac(0.2, replace = FALSE)
```

The first example using `sample_n` returned a `tibble` with 5 observations, while the second example using `sample_frac` returned a `tibble` with one-fifth of the total observations in the dataset (proportion equal to 0.2 out of 1). As always, you can save these as new datasets. The following code would save a random sample of 1/3 of the `ncbikecrash` dataset as a new dataset called `ncbikecrash_sample`:

```{r sample-3, eval = FALSE}
ncbikecrash_sample <- ncbikecrash %>%
  sample_frac(1/3, replace = FALSE)
```

Notice that here we used a fraction instead of a decimal.

### distinct and arrange

The `distinct` function returns all unique observations. Suppose we're interested in all cities and counties that had a bike crash requiring police attention. Run the following code:

```{r distinct-1, eval = FALSE}
ncbikecrash %>% 
  select(county, city)
```

As expected, we have a `tibble` containing every county/city combination in the dataset, and have only those variables. However, there are a lot of repeats! We can find the unique observations as follows:

```{r distinct-2, eval = FALSE}
ncbikecrash %>% 
  select(county, city) %>% 
  distinct()
```

"*Take the ncbikecrash data, and then select county and city, and then keep only the distinct observations*."

Note that these are in the order that they first appeared in the dataset. We can `arrange` them in alphabetical order as follows:

```{r distinct-3, eval = FALSE}
ncbikecrash %>% 
  select(county, city) %>% 
  distinct() %>% 
  arrange(county, city)
```

The above code, after taking the distinct observations, then arranges in order by county, and then by city. If we wanted to order in the opposite direction (descending, instead of ascending), we can simply use the `desc()` function:

```{r distinct-4, eval = FALSE}
ncbikecrash %>% 
  select(county, city) %>% 
  distinct() %>% 
  arrange(desc(county), city)
```

We see that we are ordering first in descending order by county, and then by ascending order by city (within the counties). If we have numeric data, we can also `arrange` to order the data based on their numeric values. The `desc()` function works the same way.

### summarise and group_by

Often times, we want to summarize values in our dataset by calculating certain summary statistics such as the mean, median, or standard deviation, etc. We can do that by using the `summarise` or `summarize` function (`dplyr` was written by Hadley Wickham, who is from New Zealand, hence the British spelling). 

Suppose we are interested in the mean hour at which crashes occur:

```{r summarize-1, eval = FALSE}
ncbikecrash %>% 
  summarize(avg_hr = mean(crash_hour))
```

Here, we're creating a variable `avg_hr` that has the mean of `crash_hour`. By running the above code, you should see that the average is 14.7, or around 2:45 PM.

We can also summarize based on different groups. For instance, if we were interested in the mean hour at which crashes occur based on whether the crash was a hit and run, we can first `group_by` the `hit_run` variable, and then summarize the mean `crash_hour` as follows:

```{r summarize-2, eval = FALSE}
ncbikecrash %>% 
  group_by(hit_run) %>% 
  summarize(avg_hr = mean(crash_hour))
```

### count

It is also easy to count observations of categorical variables. Suppose we wanted to count the number of observations in each category of the variable `driver_alcohol_drugs`. We can simply run

```{r count-1, eval = FALSE}
ncbikecrash %>% 
  count(driver_alcohol_drugs)
```

### mutate and case_when

Finally, one of the most common data manipulation tasks is to create a new variable based on values of existing variables. For instance, suppose we have a dataset named `dat`, that contains measurements of mass (`kg`), height (`m`), and body temperature in fahrenheit (`fahrenheit`) for study participants. We want to make new variables for their BMI and body temperature in celsius. We can create these variables as follows (recalling the appropriate formulas):

```{r mutate-1, eval = FALSE}
dat %>% 
  mutate(bmi = kg/m^2)

# Don't run this code; there is no "dat" dataset
```

```{r mutate-2, eval = FALSE}
dat %>% 
  mutate(celsius = (32*fahrenheit - 32) * 5/9)

# same.
```

In doing so, we've created the new variables `bmi` and `celsius` equal to the right side of the equals sign in the `mutate` function. Most often when you define a new variable with `mutate`, you'll want to use it later (for instance, in a visualization or in downstream analyses). In this case, you'd want to save the resulting dataframe, often by overwriting the original one. As an example:

```{r mutate-3, eval = FALSE}
dat <- dat %>% 
  mutate(bmi = kg/m^2)
```

In this case, we've saved over the `dat` dataframe, which now contains the new variable `bmi`.

Finally, we can also do more sophisticated data creation using `case_when`, which allows for a sequence of formulas defining new variables. For instance, let's suppose we want to create a new variable that categorizes BMI based on CDC cut-offs. Consider the following code, which utilizes the `bmi` variable we saved previously:

```{r mutate-4, eval = FALSE}
dat <- dat %>% 
  mutate(bmi_cat = case_when(
    bmi < 18 ~ "underweight",
    bmi >= 18 & bmi < 25 ~ "normal"
    bmi >= 25 & bmi < 30 ~ "oberweight"
    bmi > 30 ~ "obese"
  ))

# Be sure all parentheses are closed!
```

We have used the `case_when` function to create a new variable. The individual cases are separated by commas, and within each case, the condition (e.g., BMI > 30) is on the left and the outcome for the new variable (e.g., "obese") is on the right of a tilde (~). We have also saved over the original dataframe in order to keep this new variable, `bmi_cat`.

Let's do a concrete example using the `ncbikecrash` data. Suppose we want to classify whether a crash occurred on a weekend or on a weekday. We will use `case_when` in combination with the logical operator `%in%` to do so (the `c()` function tells `R` that there is a list of things coming up):

```{r mutate-5, eval = FALSE}
ncbikecrash <- ncbikecrash %>% 
  mutate(weekend = case_when(
    crash_day %in% c("Saturday", "Sunday") ~ "Weekend",
    crash_day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") ~ "Weekday"
  ))
```

Let's check our work by counting the number of observations in each of our categories:

```{r mutate-6, eval = FALSE}
ncbikecrash %>% 
  count(weekend)
```

It looks like 1809 crashes happened on weekends and 5658 occurred on weekdays.

# Your turn!

Wow, that was a lot of new material. Don't worry, the more you practice, the better you'll get. Try the following exercises using the `ncbikecrash` dataset. 

1. **6 points** Make sure your GitHub repository for this assignment has at 
least three commits, each with a meaningful commit message.

**Note:** each of the following exercises should be done in "one go" using a series of pipes. If you are asked to "create a table," then there is no need for additional narrative.

2. **4 points** How many bike crashes occurred in residential development areas where the driver age group was 0 to 19?

3. **4 points** Create a table which orders the most commonly occurring estimated speed (use the `driver_est_speed` variable).

4. **8 points** Consider crashes in Durham, Orange, or Wake counties. What was the mean driver age (`driver_age`) involved in a crash in each of those three counties? Display output only for those three counties. <br><br>
**Hint**: There are some missing values for `driver_age`. You can either filter for rows where the driver age isn't missing or use the `na.rm = T` option in the `mean()` function.

5. **8 points** What is the mean hour for when a crash occurs in Durham County for each month? Create a table which displays this information, arranged from earliest to latest hour.

6. **8 points** Create a new variable named `time_crash` based on the hour (`crash_hour`) that the crash occured. If the crash occured during from 12:00 am to 6:59 am, define `time_crash` to be "Early morning." If the crash occured from 7:00 am to 11:59 am, define `time_crash` to be "Morning." From 12:00 pm to 5:59 pm "Afternoon," and from 6:00 pm to 11:59 pm "Evening." Create a table which counts the number of crashes that occurred during these time periods. When do most crashes occur?

7. **12 points** Calculate the mean driver age involved in crashes in all NC counties. Create a table which displays the county name and the mean driver age in those counties. In your table, display only the top five counties in terms of oldest drivers, sorted from largest to smallest. 

**Important:** You must turn in a .pdf file corresponding to the R Markdown template to Sakai in order to receive credit for the labs and homework assignments.

# Acknowledgements

This lab was adapted from an activity created as part of Data Science in a Box, accessible at `https://datasciencebox.org/`. For more info regarding the data, contact Libby Thomas at the UNC Highway Safety Research Center at `thomas@hsrc.unc.edu` or John Vine-Hodge at the NCDOT Division of Bicycle and Pedestrian Transportation at `javinehodge@ncdot.gov`.